# Container Orchestration

## Overview

This module covers container orchestration concepts including container lifecycle management, resource scheduling, health monitoring, and scaling strategies. These concepts are essential for managing containerized applications at scale.

## Table of Contents

1. [Container Lifecycle Management](#container-lifecycle-management)
2. [Resource Scheduling](#resource-scheduling)
3. [Health Monitoring](#health-monitoring)
4. [Scaling Strategies](#scaling-strategies)
5. [Applications](#applications)
6. [Complexity Analysis](#complexity-analysis)
7. [Follow-up Questions](#follow-up-questions)

## Container Lifecycle Management

### Theory

Container lifecycle management involves creating, starting, stopping, and destroying containers. It also includes managing container dependencies and ensuring proper resource cleanup.

### Container Lifecycle Implementation

#### Golang Implementation

```go
package main

import (
    "fmt"
    "sync"
    "time"
)

type ContainerState string

const (
    Created    ContainerState = "created"
    Running    ContainerState = "running"
    Paused     ContainerState = "paused"
    Restarting ContainerState = "restarting"
    Removing   ContainerState = "removing"
    Removed    ContainerState = "removed"
    Dead       ContainerState = "dead"
)

type Container struct {
    ID          string
    Name        string
    Image       string
    State       ContainerState
    CreatedAt   time.Time
    StartedAt   time.Time
    StoppedAt   time.Time
    RestartCount int
    Resources   ResourceLimits
    Dependencies []string
    mutex       sync.RWMutex
}

type ResourceLimits struct {
    CPU    string
    Memory string
    Disk   string
}

type ContainerManager struct {
    Containers map[string]*Container
    mutex      sync.RWMutex
}

func NewContainerManager() *ContainerManager {
    return &ContainerManager{
        Containers: make(map[string]*Container),
    }
}

func (cm *ContainerManager) CreateContainer(id, name, image string, resources ResourceLimits, dependencies []string) *Container {
    container := &Container{
        ID:           id,
        Name:         name,
        Image:        image,
        State:        Created,
        CreatedAt:    time.Now(),
        RestartCount: 0,
        Resources:    resources,
        Dependencies: dependencies,
    }
    
    cm.mutex.Lock()
    cm.Containers[id] = container
    cm.mutex.Unlock()
    
    fmt.Printf("Created container: %s (%s)\n", name, id)
    return container
}

func (cm *ContainerManager) StartContainer(id string) error {
    cm.mutex.Lock()
    container, exists := cm.Containers[id]
    cm.mutex.Unlock()
    
    if !exists {
        return fmt.Errorf("container %s not found", id)
    }
    
    container.mutex.Lock()
    defer container.mutex.Unlock()
    
    if container.State != Created && container.State != Stopped {
        return fmt.Errorf("container %s is not in a startable state", id)
    }
    
    // Check dependencies
    if !cm.checkDependencies(container) {
        return fmt.Errorf("container %s dependencies not satisfied", id)
    }
    
    container.State = Running
    container.StartedAt = time.Now()
    
    fmt.Printf("Started container: %s\n", container.Name)
    return nil
}

func (cm *ContainerManager) StopContainer(id string) error {
    cm.mutex.Lock()
    container, exists := cm.Containers[id]
    cm.mutex.Unlock()
    
    if !exists {
        return fmt.Errorf("container %s not found", id)
    }
    
    container.mutex.Lock()
    defer container.mutex.Unlock()
    
    if container.State != Running {
        return fmt.Errorf("container %s is not running", id)
    }
    
    container.State = Stopped
    container.StoppedAt = time.Now()
    
    fmt.Printf("Stopped container: %s\n", container.Name)
    return nil
}

func (cm *ContainerManager) RestartContainer(id string) error {
    cm.mutex.Lock()
    container, exists := cm.Containers[id]
    cm.mutex.Unlock()
    
    if !exists {
        return fmt.Errorf("container %s not found", id)
    }
    
    container.mutex.Lock()
    container.State = Restarting
    container.RestartCount++
    container.mutex.Unlock()
    
    // Stop container
    if err := cm.StopContainer(id); err != nil {
        return err
    }
    
    // Start container
    if err := cm.StartContainer(id); err != nil {
        return err
    }
    
    fmt.Printf("Restarted container: %s (restart count: %d)\n", container.Name, container.RestartCount)
    return nil
}

func (cm *ContainerManager) PauseContainer(id string) error {
    cm.mutex.Lock()
    container, exists := cm.Containers[id]
    cm.mutex.Unlock()
    
    if !exists {
        return fmt.Errorf("container %s not found", id)
    }
    
    container.mutex.Lock()
    defer container.mutex.Unlock()
    
    if container.State != Running {
        return fmt.Errorf("container %s is not running", id)
    }
    
    container.State = Paused
    
    fmt.Printf("Paused container: %s\n", container.Name)
    return nil
}

func (cm *ContainerManager) UnpauseContainer(id string) error {
    cm.mutex.Lock()
    container, exists := cm.Containers[id]
    cm.mutex.Unlock()
    
    if !exists {
        return fmt.Errorf("container %s not found", id)
    }
    
    container.mutex.Lock()
    defer container.mutex.Unlock()
    
    if container.State != Paused {
        return fmt.Errorf("container %s is not paused", id)
    }
    
    container.State = Running
    
    fmt.Printf("Unpaused container: %s\n", container.Name)
    return nil
}

func (cm *ContainerManager) RemoveContainer(id string) error {
    cm.mutex.Lock()
    container, exists := cm.Containers[id]
    cm.mutex.Unlock()
    
    if !exists {
        return fmt.Errorf("container %s not found", id)
    }
    
    container.mutex.Lock()
    if container.State == Running {
        container.mutex.Unlock()
        return fmt.Errorf("cannot remove running container %s", id)
    }
    
    container.State = Removed
    container.mutex.Unlock()
    
    cm.mutex.Lock()
    delete(cm.Containers, id)
    cm.mutex.Unlock()
    
    fmt.Printf("Removed container: %s\n", container.Name)
    return nil
}

func (cm *ContainerManager) checkDependencies(container *Container) bool {
    for _, depID := range container.Dependencies {
        cm.mutex.RLock()
        dep, exists := cm.Containers[depID]
        cm.mutex.RUnlock()
        
        if !exists || dep.State != Running {
            return false
        }
    }
    return true
}

func (cm *ContainerManager) GetContainer(id string) *Container {
    cm.mutex.RLock()
    defer cm.mutex.RUnlock()
    
    return cm.Containers[id]
}

func (cm *ContainerManager) ListContainers() []*Container {
    cm.mutex.RLock()
    defer cm.mutex.RUnlock()
    
    containers := make([]*Container, 0, len(cm.Containers))
    for _, container := range cm.Containers {
        containers = append(containers, container)
    }
    
    return containers
}

func (cm *ContainerManager) GetContainerStats(id string) map[string]interface{} {
    cm.mutex.RLock()
    container, exists := cm.Containers[id]
    cm.mutex.RUnlock()
    
    if !exists {
        return map[string]interface{}{}
    }
    
    container.mutex.RLock()
    defer container.mutex.RUnlock()
    
    return map[string]interface{}{
        "id":            container.ID,
        "name":          container.Name,
        "image":         container.Image,
        "state":         container.State,
        "restart_count": container.RestartCount,
        "uptime":        time.Since(container.StartedAt).String(),
        "resources":     container.Resources,
    }
}

func (cm *ContainerManager) CleanupStoppedContainers() int {
    cm.mutex.Lock()
    defer cm.mutex.Unlock()
    
    count := 0
    for id, container := range cm.Containers {
        if container.State == Stopped || container.State == Dead {
            delete(cm.Containers, id)
            count++
        }
    }
    
    fmt.Printf("Cleaned up %d stopped containers\n", count)
    return count
}

func main() {
    cm := NewContainerManager()
    
    fmt.Println("Container Lifecycle Management Demo:")
    
    // Create containers
    resources := ResourceLimits{
        CPU:    "100m",
        Memory: "128Mi",
        Disk:   "1Gi",
    }
    
    container1 := cm.CreateContainer("c1", "web-server", "nginx:1.20", resources, []string{})
    container2 := cm.CreateContainer("c2", "database", "postgres:13", resources, []string{})
    container3 := cm.CreateContainer("c3", "app", "node:16", resources, []string{"c1", "c2"})
    
    // Start containers
    cm.StartContainer("c1")
    cm.StartContainer("c2")
    cm.StartContainer("c3")
    
    // Get container stats
    stats := cm.GetContainerStats("c1")
    fmt.Printf("Container stats: %v\n", stats)
    
    // Pause and unpause
    cm.PauseContainer("c1")
    cm.UnpauseContainer("c1")
    
    // Restart container
    cm.RestartContainer("c1")
    
    // List containers
    containers := cm.ListContainers()
    fmt.Printf("Total containers: %d\n", len(containers))
    
    // Stop containers
    cm.StopContainer("c1")
    cm.StopContainer("c2")
    cm.StopContainer("c3")
    
    // Cleanup
    cm.CleanupStoppedContainers()
}
```

## Resource Scheduling

### Theory

Resource scheduling involves allocating CPU, memory, and other resources to containers based on their requirements and availability. It ensures optimal resource utilization and prevents resource conflicts.

### Resource Scheduling Implementation

#### Golang Implementation

```go
package main

import (
    "fmt"
    "math"
    "sort"
    "sync"
    "time"
)

type ResourceType string

const (
    CPU    ResourceType = "cpu"
    Memory ResourceType = "memory"
    Disk   ResourceType = "disk"
    Network ResourceType = "network"
)

type Resource struct {
    Type     ResourceType
    Total    float64
    Used     float64
    Reserved float64
}

type Node struct {
    ID        string
    Name      string
    Resources map[ResourceType]*Resource
    Containers []string
    Status    string
    mutex     sync.RWMutex
}

type Scheduler struct {
    Nodes map[string]*Node
    mutex sync.RWMutex
}

func NewScheduler() *Scheduler {
    return &Scheduler{
        Nodes: make(map[string]*Node),
    }
}

func (s *Scheduler) AddNode(id, name string, resources map[ResourceType]*Resource) *Node {
    node := &Node{
        ID:        id,
        Name:      name,
        Resources: resources,
        Containers: []string{},
        Status:    "Ready",
    }
    
    s.mutex.Lock()
    s.Nodes[id] = node
    s.mutex.Unlock()
    
    fmt.Printf("Added node: %s (%s)\n", name, id)
    return node
}

func (s *Scheduler) RemoveNode(id string) bool {
    s.mutex.Lock()
    defer s.mutex.Unlock()
    
    if _, exists := s.Nodes[id]; exists {
        delete(s.Nodes, id)
        fmt.Printf("Removed node: %s\n", id)
        return true
    }
    
    return false
}

func (s *Scheduler) GetNode(id string) *Node {
    s.mutex.RLock()
    defer s.mutex.RUnlock()
    
    return s.Nodes[id]
}

func (s *Scheduler) ListNodes() []*Node {
    s.mutex.RLock()
    defer s.mutex.RUnlock()
    
    nodes := make([]*Node, 0, len(s.Nodes))
    for _, node := range s.Nodes {
        nodes = append(nodes, node)
    }
    
    return nodes
}

func (s *Scheduler) ScheduleContainer(containerID string, requirements map[ResourceType]float64) (string, error) {
    s.mutex.RLock()
    nodes := make([]*Node, 0, len(s.Nodes))
    for _, node := range s.Nodes {
        nodes = append(nodes, node)
    }
    s.mutex.RUnlock()
    
    // Find best node using different strategies
    bestNode := s.findBestNode(nodes, requirements)
    if bestNode == nil {
        return "", fmt.Errorf("no suitable node found for container %s", containerID)
    }
    
    // Allocate resources
    if err := s.allocateResources(bestNode, requirements); err != nil {
        return "", err
    }
    
    // Add container to node
    bestNode.mutex.Lock()
    bestNode.Containers = append(bestNode.Containers, containerID)
    bestNode.mutex.Unlock()
    
    fmt.Printf("Scheduled container %s to node %s\n", containerID, bestNode.Name)
    return bestNode.ID, nil
}

func (s *Scheduler) findBestNode(nodes []*Node, requirements map[ResourceType]float64) *Node {
    var bestNode *Node
    bestScore := math.Inf(-1)
    
    for _, node := range nodes {
        if s.canSchedule(node, requirements) {
            score := s.calculateScore(node, requirements)
            if score > bestScore {
                bestScore = score
                bestNode = node
            }
        }
    }
    
    return bestNode
}

func (s *Scheduler) canSchedule(node *Node, requirements map[ResourceType]float64) bool {
    node.mutex.RLock()
    defer node.mutex.RUnlock()
    
    for resourceType, required := range requirements {
        if resource, exists := node.Resources[resourceType]; exists {
            available := resource.Total - resource.Used - resource.Reserved
            if available < required {
                return false
            }
        } else {
            return false
        }
    }
    
    return true
}

func (s *Scheduler) calculateScore(node *Node, requirements map[ResourceType]float64) float64 {
    node.mutex.RLock()
    defer node.mutex.RUnlock()
    
    score := 0.0
    
    for resourceType, required := range requirements {
        if resource, exists := node.Resources[resourceType]; exists {
            available := resource.Total - resource.Used - resource.Reserved
            utilization := resource.Used / resource.Total
            score += (available - required) * (1 - utilization)
        }
    }
    
    return score
}

func (s *Scheduler) allocateResources(node *Node, requirements map[ResourceType]float64) error {
    node.mutex.Lock()
    defer node.mutex.Unlock()
    
    for resourceType, required := range requirements {
        if resource, exists := node.Resources[resourceType]; exists {
            resource.Used += required
        } else {
            return fmt.Errorf("resource type %s not available on node %s", resourceType, node.Name)
        }
    }
    
    return nil
}

func (s *Scheduler) ReleaseResources(nodeID string, requirements map[ResourceType]float64) error {
    s.mutex.RLock()
    node, exists := s.Nodes[nodeID]
    s.mutex.RUnlock()
    
    if !exists {
        return fmt.Errorf("node %s not found", nodeID)
    }
    
    node.mutex.Lock()
    defer node.mutex.Unlock()
    
    for resourceType, required := range requirements {
        if resource, exists := node.Resources[resourceType]; exists {
            resource.Used -= required
            if resource.Used < 0 {
                resource.Used = 0
            }
        }
    }
    
    fmt.Printf("Released resources from node %s\n", node.Name)
    return nil
}

func (s *Scheduler) GetNodeUtilization(nodeID string) map[ResourceType]float64 {
    s.mutex.RLock()
    node, exists := s.Nodes[nodeID]
    s.mutex.RUnlock()
    
    if !exists {
        return map[ResourceType]float64{}
    }
    
    node.mutex.RLock()
    defer node.mutex.RUnlock()
    
    utilization := make(map[ResourceType]float64)
    for resourceType, resource := range node.Resources {
        utilization[resourceType] = resource.Used / resource.Total
    }
    
    return utilization
}

func (s *Scheduler) GetClusterUtilization() map[ResourceType]float64 {
    s.mutex.RLock()
    defer s.mutex.RUnlock()
    
    totalUsed := make(map[ResourceType]float64)
    totalAvailable := make(map[ResourceType]float64)
    
    for _, node := range s.Nodes {
        node.mutex.RLock()
        for resourceType, resource := range node.Resources {
            totalUsed[resourceType] += resource.Used
            totalAvailable[resourceType] += resource.Total
        }
        node.mutex.RUnlock()
    }
    
    utilization := make(map[ResourceType]float64)
    for resourceType := range totalUsed {
        if totalAvailable[resourceType] > 0 {
            utilization[resourceType] = totalUsed[resourceType] / totalAvailable[resourceType]
        }
    }
    
    return utilization
}

func (s *Scheduler) RebalanceCluster() error {
    s.mutex.RLock()
    nodes := make([]*Node, 0, len(s.Nodes))
    for _, node := range s.Nodes {
        nodes = append(nodes, node)
    }
    s.mutex.RUnlock()
    
    // Sort nodes by utilization
    sort.Slice(nodes, func(i, j int) bool {
        utilI := s.GetNodeUtilization(nodes[i].ID)
        utilJ := s.GetNodeUtilization(nodes[j].ID)
        
        avgUtilI := 0.0
        avgUtilJ := 0.0
        
        for _, util := range utilI {
            avgUtilI += util
        }
        for _, util := range utilJ {
            avgUtilJ += util
        }
        
        return avgUtilI > avgUtilJ
    })
    
    fmt.Printf("Rebalanced cluster with %d nodes\n", len(nodes))
    return nil
}

func main() {
    scheduler := NewScheduler()
    
    fmt.Println("Resource Scheduling Demo:")
    
    // Add nodes
    node1Resources := map[ResourceType]*Resource{
        CPU:    {Type: CPU, Total: 4.0, Used: 0, Reserved: 0},
        Memory: {Type: Memory, Total: 8192, Used: 0, Reserved: 0},
        Disk:   {Type: Disk, Total: 100, Used: 0, Reserved: 0},
    }
    
    node2Resources := map[ResourceType]*Resource{
        CPU:    {Type: CPU, Total: 2.0, Used: 0, Reserved: 0},
        Memory: {Type: Memory, Total: 4096, Used: 0, Reserved: 0},
        Disk:   {Type: Disk, Total: 50, Used: 0, Reserved: 0},
    }
    
    scheduler.AddNode("n1", "node-1", node1Resources)
    scheduler.AddNode("n2", "node-2", node2Resources)
    
    // Schedule containers
    requirements1 := map[ResourceType]float64{
        CPU:    1.0,
        Memory: 1024,
        Disk:   10,
    }
    
    requirements2 := map[ResourceType]float64{
        CPU:    0.5,
        Memory: 512,
        Disk:   5,
    }
    
    nodeID1, err := scheduler.ScheduleContainer("c1", requirements1)
    if err != nil {
        fmt.Printf("Error scheduling container c1: %v\n", err)
    } else {
        fmt.Printf("Container c1 scheduled to node: %s\n", nodeID1)
    }
    
    nodeID2, err := scheduler.ScheduleContainer("c2", requirements2)
    if err != nil {
        fmt.Printf("Error scheduling container c2: %v\n", err)
    } else {
        fmt.Printf("Container c2 scheduled to node: %s\n", nodeID2)
    }
    
    // Get utilization
    util1 := scheduler.GetNodeUtilization("n1")
    fmt.Printf("Node 1 utilization: %v\n", util1)
    
    clusterUtil := scheduler.GetClusterUtilization()
    fmt.Printf("Cluster utilization: %v\n", clusterUtil)
    
    // Rebalance
    scheduler.RebalanceCluster()
}
```

## Follow-up Questions

### 1. Container Lifecycle Management
**Q: What are the different container states and how do they transition?**
A: Container states include created, running, paused, restarting, removing, removed, and dead. Transitions depend on operations like start, stop, pause, restart, and remove.

### 2. Resource Scheduling
**Q: What factors should be considered when scheduling containers?**
A: Consider resource requirements, node capacity, current utilization, affinity rules, anti-affinity rules, and load balancing across nodes.

### 3. Health Monitoring
**Q: What are the different types of health checks for containers?**
A: Health checks include liveness probes (is container running?), readiness probes (is container ready to serve traffic?), and startup probes (is container started?).

## Complexity Analysis

| Operation | Container Lifecycle | Resource Scheduling | Health Monitoring |
|-----------|-------------------|-------------------|------------------|
| Create | O(1) | O(1) | O(1) |
| Start | O(1) | O(n) | O(1) |
| Stop | O(1) | O(1) | O(1) |
| Schedule | N/A | O(n log n) | N/A |
| Monitor | N/A | N/A | O(n) |

## Applications

1. **Container Lifecycle Management**: Container orchestration, CI/CD pipelines, microservices deployment
2. **Resource Scheduling**: Cloud computing, resource optimization, load balancing
3. **Health Monitoring**: Service reliability, automated recovery, performance monitoring
4. **Container Orchestration**: Kubernetes, Docker Swarm, cloud-native applications

---

**Next**: [Serverless Architecture](serverless-architecture.md) | **Previous**: [Cloud Architecture](README.md) | **Up**: [Cloud Architecture](README.md)
