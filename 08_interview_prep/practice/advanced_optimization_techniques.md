# Advanced Optimization Techniques

## Table of Contents
- [Introduction](#introduction/)
- [Algorithm Optimization](#algorithm-optimization/)
- [Memory Optimization](#memory-optimization/)
- [Concurrency Optimization](#concurrency-optimization/)
- [Database Optimization](#database-optimization/)
- [Network Optimization](#network-optimization/)
- [Caching Strategies](#caching-strategies/)
- [Performance Profiling](#performance-profiling/)

## Introduction

Advanced optimization techniques are essential for building high-performance systems that can handle massive scale and real-world constraints.

## Algorithm Optimization

### Time Complexity Optimization

```go
// O(nÂ²) to O(n) optimization
func FindTwoSumBruteForce(nums []int, target int) []int {
    for i := 0; i < len(nums); i++ {
        for j := i + 1; j < len(nums); j++ {
            if nums[i]+nums[j] == target {
                return []int{i, j}
            }
        }
    }
    return nil
}

func FindTwoSumOptimized(nums []int, target int) []int {
    numMap := make(map[int]int)
    for i, num := range nums {
        complement := target - num
        if index, exists := numMap[complement]; exists {
            return []int{index, i}
        }
        numMap[num] = i
    }
    return nil
}

// Space-time tradeoff optimization
func FindDuplicatesSpaceOptimized(nums []int) []int {
    var duplicates []int
    
    for i := 0; i < len(nums); i++ {
        index := abs(nums[i]) - 1
        if nums[index] < 0 {
            duplicates = append(duplicates, abs(nums[i]))
        } else {
            nums[index] = -nums[index]
        }
    }
    
    return duplicates
}

func abs(x int) int {
    if x < 0 {
        return -x
    }
    return x
}
```

### Dynamic Programming Optimization

```go
// Memoization for Fibonacci
func FibonacciMemoized(n int) int {
    memo := make(map[int]int)
    return fibonacciMemo(n, memo)
}

func fibonacciMemo(n int, memo map[int]int) int {
    if n <= 1 {
        return n
    }
    
    if val, exists := memo[n]; exists {
        return val
    }
    
    memo[n] = fibonacciMemo(n-1, memo) + fibonacciMemo(n-2, memo)
    return memo[n]
}

// Tabulation for Fibonacci (space optimized)
func FibonacciTabulated(n int) int {
    if n <= 1 {
        return n
    }
    
    prev2, prev1 := 0, 1
    for i := 2; i <= n; i++ {
        current := prev1 + prev2
        prev2 = prev1
        prev1 = current
    }
    
    return prev1
}

// Longest Common Subsequence with space optimization
func LCSOptimized(s1, s2 string) int {
    m, n := len(s1), len(s2)
    if m < n {
        s1, s2 = s2, s1
        m, n = n, m
    }
    
    prev := make([]int, n+1)
    curr := make([]int, n+1)
    
    for i := 1; i <= m; i++ {
        for j := 1; j <= n; j++ {
            if s1[i-1] == s2[j-1] {
                curr[j] = prev[j-1] + 1
            } else {
                curr[j] = max(prev[j], curr[j-1])
            }
        }
        prev, curr = curr, prev
    }
    
    return prev[n]
}
```

### Bit Manipulation Optimization

```go
// Fast power calculation using bit manipulation
func FastPower(base, exponent int) int {
    result := 1
    for exponent > 0 {
        if exponent&1 == 1 {
            result *= base
        }
        base *= base
        exponent >>= 1
    }
    return result
}

// Count set bits (Hamming weight)
func CountSetBits(n int) int {
    count := 0
    for n > 0 {
        count++
        n &= n - 1 // Remove the rightmost set bit
    }
    return count
}

// Find single number in array where all others appear twice
func FindSingleNumber(nums []int) int {
    result := 0
    for _, num := range nums {
        result ^= num
    }
    return result
}

// Check if number is power of 2
func IsPowerOfTwo(n int) bool {
    return n > 0 && (n&(n-1)) == 0
}
```

## Memory Optimization

### Memory Pool Pattern

```go
// Memory pool for reducing GC pressure
type MemoryPool struct {
    pool    sync.Pool
    maxSize int
}

type Buffer struct {
    data []byte
    pool *MemoryPool
}

func NewMemoryPool(maxSize int) *MemoryPool {
    return &MemoryPool{
        maxSize: maxSize,
        pool: sync.Pool{
            New: func() interface{} {
                return &Buffer{
                    data: make([]byte, 0, maxSize),
                }
            },
        },
    }
}

func (mp *MemoryPool) Get() *Buffer {
    buf := mp.pool.Get().(*Buffer)
    buf.data = buf.data[:0] // Reset length but keep capacity
    return buf
}

func (mp *MemoryPool) Put(buf *Buffer) {
    if cap(buf.data) <= mp.maxSize {
        mp.pool.Put(buf)
    }
}

func (b *Buffer) Write(data []byte) {
    b.data = append(b.data, data...)
}

func (b *Buffer) Bytes() []byte {
    return b.data
}

func (b *Buffer) Reset() {
    b.data = b.data[:0]
}
```

### String Optimization

```go
// String builder for efficient string concatenation
type StringBuilder struct {
    data []byte
}

func NewStringBuilder() *StringBuilder {
    return &StringBuilder{
        data: make([]byte, 0, 64),
    }
}

func (sb *StringBuilder) WriteString(s string) {
    sb.data = append(sb.data, s...)
}

func (sb *StringBuilder) WriteByte(c byte) {
    sb.data = append(sb.data, c)
}

func (sb *StringBuilder) WriteRune(r rune) {
    sb.data = append(sb.data, string(r)...)
}

func (sb *StringBuilder) String() string {
    return string(sb.data)
}

func (sb *StringBuilder) Reset() {
    sb.data = sb.data[:0]
}

func (sb *StringBuilder) Len() int {
    return len(sb.data)
}

func (sb *StringBuilder) Cap() int {
    return cap(sb.data)
}

// String interning to reduce memory usage
type StringInterner struct {
    strings map[string]string
    mutex   sync.RWMutex
}

func NewStringInterner() *StringInterner {
    return &StringInterner{
        strings: make(map[string]string),
    }
}

func (si *StringInterner) Intern(s string) string {
    si.mutex.RLock()
    if interned, exists := si.strings[s]; exists {
        si.mutex.RUnlock()
        return interned
    }
    si.mutex.RUnlock()
    
    si.mutex.Lock()
    defer si.mutex.Unlock()
    
    if interned, exists := si.strings[s]; exists {
        return interned
    }
    
    si.strings[s] = s
    return s
}
```

### Slice Optimization

```go
// Pre-allocated slice for known capacity
func ProcessItems(items []Item) []Result {
    results := make([]Result, 0, len(items)) // Pre-allocate with known capacity
    
    for _, item := range items {
        result := processItem(item)
        results = append(results, result)
    }
    
    return results
}

// Slice reuse to reduce allocations
type SliceReuser struct {
    slices [][]int
    index  int
    mutex  sync.Mutex
}

func NewSliceReuser() *SliceReuser {
    return &SliceReuser{
        slices: make([][]int, 0, 10),
    }
}

func (sr *SliceReuser) Get() []int {
    sr.mutex.Lock()
    defer sr.mutex.Unlock()
    
    if sr.index >= len(sr.slices) {
        sr.slices = append(sr.slices, make([]int, 0, 100))
    }
    
    slice := sr.slices[sr.index]
    sr.index++
    return slice[:0] // Reset length but keep capacity
}

func (sr *SliceReuser) Put(slice []int) {
    sr.mutex.Lock()
    defer sr.mutex.Unlock()
    
    if sr.index > 0 {
        sr.index--
    }
}
```

## Concurrency Optimization

### Lock-Free Data Structures

```go
// Lock-free stack using atomic operations
type LockFreeStack struct {
    head unsafe.Pointer
}

type node struct {
    value interface{}
    next  unsafe.Pointer
}

func NewLockFreeStack() *LockFreeStack {
    return &LockFreeStack{}
}

func (s *LockFreeStack) Push(value interface{}) {
    n := &node{value: value}
    
    for {
        head := atomic.LoadPointer(&s.head)
        n.next = head
        
        if atomic.CompareAndSwapPointer(&s.head, head, unsafe.Pointer(n)) {
            break
        }
    }
}

func (s *LockFreeStack) Pop() interface{} {
    for {
        head := atomic.LoadPointer(&s.head)
        if head == nil {
            return nil
        }
        
        n := (*node)(head)
        next := atomic.LoadPointer(&n.next)
        
        if atomic.CompareAndSwapPointer(&s.head, head, next) {
            return n.value
        }
    }
}

// Lock-free counter
type LockFreeCounter struct {
    value int64
}

func NewLockFreeCounter() *LockFreeCounter {
    return &LockFreeCounter{}
}

func (c *LockFreeCounter) Increment() {
    atomic.AddInt64(&c.value, 1)
}

func (c *LockFreeCounter) Decrement() {
    atomic.AddInt64(&c.value, -1)
}

func (c *LockFreeCounter) Value() int64 {
    return atomic.LoadInt64(&c.value)
}

func (c *LockFreeCounter) CompareAndSwap(old, new int64) bool {
    return atomic.CompareAndSwapInt64(&c.value, old, new)
}
```

### Worker Pool Optimization

```go
// Optimized worker pool with work stealing
type WorkStealingPool struct {
    workers    []*Worker
    numWorkers int
    workQueues []chan Work
    done       chan bool
}

type Worker struct {
    id       int
    workQueue chan Work
    pool     *WorkStealingPool
}

type Work struct {
    ID   int
    Data interface{}
    Fn   func(interface{}) interface{}
}

func NewWorkStealingPool(numWorkers int) *WorkStealingPool {
    pool := &WorkStealingPool{
        numWorkers: numWorkers,
        workQueues: make([]chan Work, numWorkers),
        done:       make(chan bool),
    }
    
    pool.workers = make([]*Worker, numWorkers)
    for i := 0; i < numWorkers; i++ {
        pool.workQueues[i] = make(chan Work, 100)
        pool.workers[i] = &Worker{
            id:        i,
            workQueue: pool.workQueues[i],
            pool:      pool,
        }
    }
    
    return pool
}

func (p *WorkStealingPool) Start() {
    for _, worker := range p.workers {
        go worker.run()
    }
}

func (w *Worker) run() {
    for {
        select {
        case work := <-w.workQueue:
            w.execute(work)
        case <-w.pool.done:
            return
        default:
            // Try to steal work from other workers
            if w.stealWork() {
                continue
            }
            // No work available, yield
            runtime.Gosched()
        }
    }
}

func (w *Worker) execute(work Work) {
    result := work.Fn(work.Data)
    // Process result...
    _ = result
}

func (w *Worker) stealWork() bool {
    for i := 0; i < w.pool.numWorkers; i++ {
        targetID := (w.id + i + 1) % w.pool.numWorkers
        if targetID == w.id {
            continue
        }
        
        select {
        case work := <-w.pool.workQueues[targetID]:
            w.execute(work)
            return true
        default:
            continue
        }
    }
    return false
}

func (p *WorkStealingPool) Submit(work Work) {
    // Distribute work round-robin
    workerID := work.ID % p.numWorkers
    select {
    case p.workQueues[workerID] <- work:
    default:
        // Queue full, try another worker
        for i := 0; i < p.numWorkers; i++ {
            targetID := (workerID + i) % p.numWorkers
            select {
            case p.workQueues[targetID] <- work:
                return
            default:
                continue
            }
        }
    }
}

func (p *WorkStealingPool) Stop() {
    close(p.done)
}
```

## Database Optimization

### Connection Pooling

```go
// Database connection pool
type ConnectionPool struct {
    connections chan *sql.DB
    factory     func() (*sql.DB, error)
    maxSize     int
    currentSize int
    mutex       sync.Mutex
}

func NewConnectionPool(factory func() (*sql.DB, error), maxSize int) *ConnectionPool {
    return &ConnectionPool{
        connections: make(chan *sql.DB, maxSize),
        factory:     factory,
        maxSize:     maxSize,
    }
}

func (p *ConnectionPool) Get() (*sql.DB, error) {
    select {
    case conn := <-p.connections:
        return conn, nil
    default:
        p.mutex.Lock()
        defer p.mutex.Unlock()
        
        if p.currentSize < p.maxSize {
            conn, err := p.factory()
            if err != nil {
                return nil, err
            }
            p.currentSize++
            return conn, nil
        }
        
        // Wait for available connection
        return <-p.connections, nil
    }
}

func (p *ConnectionPool) Put(conn *sql.DB) {
    select {
    case p.connections <- conn:
    default:
        // Pool is full, close connection
        conn.Close()
        p.mutex.Lock()
        p.currentSize--
        p.mutex.Unlock()
    }
}

func (p *ConnectionPool) Close() {
    close(p.connections)
    for conn := range p.connections {
        conn.Close()
    }
}
```

### Query Optimization

```go
// Prepared statement cache
type PreparedStatementCache struct {
    statements map[string]*sql.Stmt
    mutex      sync.RWMutex
}

func NewPreparedStatementCache() *PreparedStatementCache {
    return &PreparedStatementCache{
        statements: make(map[string]*sql.Stmt),
    }
}

func (c *PreparedStatementCache) Get(db *sql.DB, query string) (*sql.Stmt, error) {
    c.mutex.RLock()
    if stmt, exists := c.statements[query]; exists {
        c.mutex.RUnlock()
        return stmt, nil
    }
    c.mutex.RUnlock()
    
    c.mutex.Lock()
    defer c.mutex.Unlock()
    
    if stmt, exists := c.statements[query]; exists {
        return stmt, nil
    }
    
    stmt, err := db.Prepare(query)
    if err != nil {
        return nil, err
    }
    
    c.statements[query] = stmt
    return stmt, nil
}

// Batch operations
func BatchInsert(db *sql.DB, table string, columns []string, values [][]interface{}) error {
    if len(values) == 0 {
        return nil
    }
    
    query := fmt.Sprintf("INSERT INTO %s (%s) VALUES %s",
        table,
        strings.Join(columns, ","),
        generatePlaceholders(len(columns), len(values)))
    
    stmt, err := db.Prepare(query)
    if err != nil {
        return err
    }
    defer stmt.Close()
    
    // Flatten values for batch insert
    flatValues := make([]interface{}, 0, len(columns)*len(values))
    for _, row := range values {
        flatValues = append(flatValues, row...)
    }
    
    _, err = stmt.Exec(flatValues...)
    return err
}

func generatePlaceholders(columns, rows int) string {
    placeholders := make([]string, rows)
    for i := 0; i < rows; i++ {
        rowPlaceholders := make([]string, columns)
        for j := 0; j < columns; j++ {
            rowPlaceholders[j] = "?"
        }
        placeholders[i] = "(" + strings.Join(rowPlaceholders, ",") + ")"
    }
    return strings.Join(placeholders, ",")
}
```

## Network Optimization

### Connection Reuse

```go
// HTTP client with connection pooling
type OptimizedHTTPClient struct {
    client *http.Client
    transport *http.Transport
}

func NewOptimizedHTTPClient() *OptimizedHTTPClient {
    transport := &http.Transport{
        MaxIdleConns:        100,
        MaxIdleConnsPerHost: 10,
        IdleConnTimeout:     90 * time.Second,
        DisableKeepAlives:   false,
    }
    
    client := &http.Client{
        Transport: transport,
        Timeout:   30 * time.Second,
    }
    
    return &OptimizedHTTPClient{
        client:    client,
        transport: transport,
    }
}

func (c *OptimizedHTTPClient) Get(url string) (*http.Response, error) {
    return c.client.Get(url)
}

func (c *OptimizedHTTPClient) Post(url string, body io.Reader) (*http.Response, error) {
    return c.client.Post(url, "application/json", body)
}

func (c *OptimizedHTTPClient) Close() {
    c.transport.CloseIdleConnections()
}
```

### Message Batching

```go
// Message batcher for network optimization
type MessageBatcher struct {
    messages   chan Message
    batchSize  int
    flushTime  time.Duration
    processor  func([]Message) error
    done       chan bool
}

type Message struct {
    ID   string
    Data interface{}
}

func NewMessageBatcher(batchSize int, flushTime time.Duration, processor func([]Message) error) *MessageBatcher {
    return &MessageBatcher{
        messages:  make(chan Message, batchSize*2),
        batchSize: batchSize,
        flushTime: flushTime,
        processor: processor,
        done:      make(chan bool),
    }
}

func (mb *MessageBatcher) Start() {
    go mb.batchProcessor()
}

func (mb *MessageBatcher) batchProcessor() {
    batch := make([]Message, 0, mb.batchSize)
    ticker := time.NewTicker(mb.flushTime)
    defer ticker.Stop()
    
    for {
        select {
        case msg := <-mb.messages:
            batch = append(batch, msg)
            if len(batch) >= mb.batchSize {
                mb.flushBatch(batch)
                batch = batch[:0]
            }
        case <-ticker.C:
            if len(batch) > 0 {
                mb.flushBatch(batch)
                batch = batch[:0]
            }
        case <-mb.done:
            if len(batch) > 0 {
                mb.flushBatch(batch)
            }
            return
        }
    }
}

func (mb *MessageBatcher) flushBatch(batch []Message) {
    if err := mb.processor(batch); err != nil {
        // Handle error...
        fmt.Printf("Error processing batch: %v\n", err)
    }
}

func (mb *MessageBatcher) AddMessage(msg Message) {
    select {
    case mb.messages <- msg:
    default:
        // Channel full, handle overflow
        fmt.Println("Message batcher queue full")
    }
}

func (mb *MessageBatcher) Stop() {
    close(mb.done)
}
```

## Caching Strategies

### Multi-Level Caching

```go
// Multi-level cache implementation
type MultiLevelCache struct {
    l1Cache *sync.Map // In-memory cache
    l2Cache Cache     // Redis cache
    l3Cache Cache     // Database cache
}

type Cache interface {
    Get(key string) (interface{}, error)
    Set(key string, value interface{}, ttl time.Duration) error
    Delete(key string) error
}

func NewMultiLevelCache(l2Cache, l3Cache Cache) *MultiLevelCache {
    return &MultiLevelCache{
        l1Cache: &sync.Map{},
        l2Cache: l2Cache,
        l3Cache: l3Cache,
    }
}

func (mlc *MultiLevelCache) Get(key string) (interface{}, error) {
    // L1 Cache (in-memory)
    if value, exists := mlc.l1Cache.Load(key); exists {
        return value, nil
    }
    
    // L2 Cache (Redis)
    if value, err := mlc.l2Cache.Get(key); err == nil {
        mlc.l1Cache.Store(key, value)
        return value, nil
    }
    
    // L3 Cache (Database)
    if value, err := mlc.l3Cache.Get(key); err == nil {
        mlc.l2Cache.Set(key, value, time.Hour)
        mlc.l1Cache.Store(key, value)
        return value, nil
    }
    
    return nil, fmt.Errorf("key not found: %s", key)
}

func (mlc *MultiLevelCache) Set(key string, value interface{}, ttl time.Duration) error {
    // Set in all levels
    mlc.l1Cache.Store(key, value)
    mlc.l2Cache.Set(key, value, ttl)
    mlc.l3Cache.Set(key, value, ttl*2) // Longer TTL for L3
    
    return nil
}

func (mlc *MultiLevelCache) Delete(key string) error {
    mlc.l1Cache.Delete(key)
    mlc.l2Cache.Delete(key)
    mlc.l3Cache.Delete(key)
    return nil
}
```

### Cache-Aside Pattern

```go
// Cache-aside pattern implementation
type CacheAsideService struct {
    cache Cache
    db    Database
}

func (cas *CacheAsideService) GetUser(id int) (*User, error) {
    key := fmt.Sprintf("user:%d", id)
    
    // Try cache first
    if cached, err := cas.cache.Get(key); err == nil {
        if user, ok := cached.(*User); ok {
            return user, nil
        }
    }
    
    // Cache miss, get from database
    user, err := cas.db.GetUser(id)
    if err != nil {
        return nil, err
    }
    
    // Store in cache
    cas.cache.Set(key, user, time.Hour)
    
    return user, nil
}

func (cas *CacheAsideService) UpdateUser(user *User) error {
    // Update database
    if err := cas.db.UpdateUser(user); err != nil {
        return err
    }
    
    // Invalidate cache
    key := fmt.Sprintf("user:%d", user.ID)
    cas.cache.Delete(key)
    
    return nil
}
```

## Performance Profiling

### CPU Profiling

```go
// CPU profiling utilities
func StartCPUProfile(filename string) (*os.File, error) {
    f, err := os.Create(filename)
    if err != nil {
        return nil, err
    }
    
    if err := pprof.StartCPUProfile(f); err != nil {
        f.Close()
        return nil, err
    }
    
    return f, nil
}

func StopCPUProfile(f *os.File) {
    pprof.StopCPUProfile()
    f.Close()
}

// Memory profiling
func WriteMemProfile(filename string) error {
    f, err := os.Create(filename)
    if err != nil {
        return err
    }
    defer f.Close()
    
    return pprof.WriteHeapProfile(f)
}

// Goroutine profiling
func WriteGoroutineProfile(filename string) error {
    f, err := os.Create(filename)
    if err != nil {
        return err
    }
    defer f.Close()
    
    return pprof.Lookup("goroutine").WriteTo(f, 0)
}
```

### Benchmarking

```go
// Benchmark utilities
func BenchmarkFunction(b *testing.B, fn func()) {
    b.ResetTimer()
    for i := 0; i < b.N; i++ {
        fn()
    }
}

func BenchmarkWithSetup(b *testing.B, setup func(), fn func(), cleanup func()) {
    for i := 0; i < b.N; i++ {
        setup()
        fn()
        cleanup()
    }
}

// Memory allocation benchmarking
func BenchmarkMemoryAllocation(b *testing.B) {
    b.ReportAllocs()
    
    for i := 0; i < b.N; i++ {
        // Test memory allocation patterns
        data := make([]byte, 1024)
        _ = data
    }
}

// Concurrent benchmarking
func BenchmarkConcurrent(b *testing.B) {
    b.RunParallel(func(pb *testing.PB) {
        for pb.Next() {
            // Test concurrent operations
            processData()
        }
    })
}
```

## Conclusion

Advanced optimization techniques include:

1. **Algorithm Optimization**: Improving time and space complexity
2. **Memory Optimization**: Reducing allocations and GC pressure
3. **Concurrency Optimization**: Maximizing parallel processing
4. **Database Optimization**: Efficient data access patterns
5. **Network Optimization**: Minimizing latency and bandwidth
6. **Caching Strategies**: Multi-level caching and cache patterns
7. **Performance Profiling**: Measuring and analyzing performance

Mastering these techniques demonstrates your ability to build high-performance, scalable systems that can handle real-world production loads.

## Additional Resources

- [Performance Optimization](https://www.performanceoptimization.com/)
- [Memory Management](https://www.memorymanagement.com/)
- [Concurrency Optimization](https://www.concurrencyoptimization.com/)
- [Database Optimization](https://www.databaseoptimization.com/)
- [Network Optimization](https://www.networkoptimization.com/)
- [Caching Strategies](https://www.cachingstrategies.com/)
- [Performance Profiling](https://www.performanceprofiling.com/)
