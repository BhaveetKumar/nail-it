# Advanced ML Systems

Advanced machine learning systems for backend engineers.

## 🎯 ML System Architecture

### Microservices ML Architecture
```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Data Ingestion│    │  Model Training │    │  Model Serving  │
│   (Kafka)       │────│   (Kubernetes)  │────│   (Seldon)      │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                                │
                       ┌─────────────────┐
                       │  Model Registry │
                       │   (MLflow)      │
                       └─────────────────┘
```

### Real-time ML Pipeline
```go
// Real-time feature serving
type FeatureStore struct {
    redis     *redis.Client
    postgres  *gorm.DB
    cache     *cache.Cache
}

func (fs *FeatureStore) GetFeatures(entityID string, featureNames []string) (map[string]interface{}, error) {
    // Check cache first
    cacheKey := fmt.Sprintf("features:%s:%v", entityID, featureNames)
    if cached, found := fs.cache.Get(cacheKey); found {
        return cached.(map[string]interface{}), nil
    }
    
    // Get from Redis
    features := make(map[string]interface{})
    for _, name := range featureNames {
        key := fmt.Sprintf("feature:%s:%s", entityID, name)
        value, err := fs.redis.Get(key).Result()
        if err == nil {
            features[name] = value
        }
    }
    
    // Cache result
    fs.cache.Set(cacheKey, features, 5*time.Minute)
    
    return features, nil
}
```

## 🔧 Model Serving

### Model Server Implementation
```go
type ModelServer struct {
    models    map[string]Model
    predictor *Predictor
    cache     *cache.Cache
}

type Model interface {
    Predict(features map[string]interface{}) (interface{}, error)
    GetVersion() string
    IsHealthy() bool
}

func (ms *ModelServer) Predict(modelName string, features map[string]interface{}) (interface{}, error) {
    // Check cache
    cacheKey := fmt.Sprintf("prediction:%s:%v", modelName, features)
    if cached, found := ms.cache.Get(cacheKey); found {
        return cached, nil
    }
    
    // Get model
    model, exists := ms.models[modelName]
    if !exists {
        return nil, errors.New("model not found")
    }
    
    // Make prediction
    prediction, err := model.Predict(features)
    if err != nil {
        return nil, err
    }
    
    // Cache result
    ms.cache.Set(cacheKey, prediction, 1*time.Minute)
    
    return prediction, nil
}
```

### A/B Testing for ML Models
```go
type ABTestManager struct {
    experiments map[string]*Experiment
    random      *rand.Rand
}

type Experiment struct {
    ID          string
    Name        string
    Models      map[string]Model
    TrafficSplit map[string]float64
    StartDate   time.Time
    EndDate     time.Time
    Status      string
}

func (ab *ABTestManager) GetModel(userID string, experimentID string) (Model, error) {
    experiment, exists := ab.experiments[experimentID]
    if !exists {
        return nil, errors.New("experiment not found")
    }
    
    // Determine which model to use based on traffic split
    userHash := hash(userID)
    cumulative := 0.0
    
    for modelName, split := range experiment.TrafficSplit {
        cumulative += split
        if userHash < cumulative {
            return experiment.Models[modelName], nil
        }
    }
    
    // Default to first model
    for _, model := range experiment.Models {
        return model, nil
    }
    
    return nil, errors.New("no models available")
}
```

## 📊 MLOps Pipeline

### Model Training Pipeline
```go
type TrainingPipeline struct {
    dataSource  DataSource
    preprocessor Preprocessor
    trainer     Trainer
    evaluator   Evaluator
    registry    ModelRegistry
}

func (tp *TrainingPipeline) Train(config TrainingConfig) (*Model, error) {
    // Load data
    data, err := tp.dataSource.Load(config.DatasetPath)
    if err != nil {
        return nil, err
    }
    
    // Preprocess data
    processedData, err := tp.preprocessor.Process(data)
    if err != nil {
        return nil, err
    }
    
    // Split data
    trainData, valData, testData := tp.splitData(processedData)
    
    // Train model
    model, err := tp.trainer.Train(trainData, config.Hyperparameters)
    if err != nil {
        return nil, err
    }
    
    // Evaluate model
    metrics, err := tp.evaluator.Evaluate(model, valData, testData)
    if err != nil {
        return nil, err
    }
    
    // Register model
    modelVersion := &ModelVersion{
        Model:     model,
        Metrics:   metrics,
        Timestamp: time.Now(),
    }
    
    if err := tp.registry.Register(modelVersion); err != nil {
        return nil, err
    }
    
    return model, nil
}
```

### Model Monitoring
```go
type ModelMonitor struct {
    metrics    *prometheus.Registry
    alerting   *AlertingService
    thresholds map[string]float64
}

func (mm *ModelMonitor) MonitorModel(modelName string, predictions []Prediction) error {
    // Calculate model metrics
    accuracy := mm.calculateAccuracy(predictions)
    latency := mm.calculateLatency(predictions)
    throughput := mm.calculateThroughput(predictions)
    
    // Update metrics
    mm.updateMetrics(modelName, accuracy, latency, throughput)
    
    // Check thresholds
    if accuracy < mm.thresholds["accuracy"] {
        mm.alerting.SendAlert("Model accuracy below threshold", map[string]interface{}{
            "model":    modelName,
            "accuracy": accuracy,
            "threshold": mm.thresholds["accuracy"],
        })
    }
    
    return nil
}
```

## 🔍 Feature Engineering

### Real-time Feature Engineering
```go
type FeatureEngineer struct {
    featureStore FeatureStore
    processors   map[string]FeatureProcessor
}

type FeatureProcessor interface {
    Process(entityID string, data map[string]interface{}) (map[string]interface{}, error)
}

func (fe *FeatureEngineer) ProcessFeatures(entityID string, rawData map[string]interface{}) (map[string]interface{}, error) {
    features := make(map[string]interface{})
    
    // Process each feature
    for name, processor := range fe.processors {
        processed, err := processor.Process(entityID, rawData)
        if err != nil {
            return nil, err
        }
        
        // Merge processed features
        for k, v := range processed {
            features[k] = v
        }
    }
    
    // Store features
    if err := fe.featureStore.StoreFeatures(entityID, features); err != nil {
        return nil, err
    }
    
    return features, nil
}
```

### Feature Validation
```go
type FeatureValidator struct {
    schemas map[string]FeatureSchema
}

type FeatureSchema struct {
    Name        string
    Type        string
    MinValue    *float64
    MaxValue    *float64
    AllowedValues []interface{}
    Required    bool
}

func (fv *FeatureValidator) Validate(features map[string]interface{}) error {
    for name, value := range features {
        schema, exists := fv.schemas[name]
        if !exists {
            continue
        }
        
        if err := fv.validateFeature(name, value, schema); err != nil {
            return err
        }
    }
    
    return nil
}

func (fv *FeatureValidator) validateFeature(name string, value interface{}, schema FeatureSchema) error {
    // Check required
    if schema.Required && value == nil {
        return fmt.Errorf("required feature %s is missing", name)
    }
    
    // Check type
    if err := fv.checkType(value, schema.Type); err != nil {
        return err
    }
    
    // Check range
    if err := fv.checkRange(value, schema); err != nil {
        return err
    }
    
    // Check allowed values
    if err := fv.checkAllowedValues(value, schema); err != nil {
        return err
    }
    
    return nil
}
```

## 🚀 Model Deployment

### Canary Deployment
```go
type CanaryDeployment struct {
    modelRegistry ModelRegistry
    trafficSplit  map[string]float64
    healthCheck   HealthChecker
}

func (cd *CanaryDeployment) DeployModel(modelVersion *ModelVersion) error {
    // Deploy to canary environment
    if err := cd.deployToCanary(modelVersion); err != nil {
        return err
    }
    
    // Start with small traffic split
    cd.trafficSplit[modelVersion.ID] = 0.1
    
    // Monitor canary
    go cd.monitorCanary(modelVersion)
    
    return nil
}

func (cd *CanaryDeployment) monitorCanary(modelVersion *ModelVersion) {
    ticker := time.NewTicker(5 * time.Minute)
    defer ticker.Stop()
    
    for i := 0; i < 12; i++ { // Monitor for 1 hour
        <-ticker.C
        
        // Check health
        if !cd.healthCheck.IsHealthy(modelVersion.ID) {
            // Rollback canary
            cd.rollbackCanary(modelVersion)
            return
        }
        
        // Increase traffic split
        if i%3 == 0 { // Every 15 minutes
            cd.trafficSplit[modelVersion.ID] = math.Min(1.0, cd.trafficSplit[modelVersion.ID]+0.1)
        }
    }
    
    // Promote to production
    cd.promoteToProduction(modelVersion)
}
```

### Model Versioning
```go
type ModelVersion struct {
    ID          string
    Model       Model
    Metrics     map[string]float64
    Timestamp   time.Time
    Status      string
    Description string
}

type ModelRegistry struct {
    db *gorm.DB
}

func (mr *ModelRegistry) Register(version *ModelVersion) error {
    version.ID = generateVersionID()
    version.Status = "staging"
    
    return mr.db.Create(version).Error
}

func (mr *ModelRegistry) Promote(versionID string) error {
    // Update status to production
    return mr.db.Model(&ModelVersion{}).
        Where("id = ?", versionID).
        Update("status", "production").Error
}

func (mr *ModelRegistry) GetProductionModel(modelName string) (*ModelVersion, error) {
    var version ModelVersion
    err := mr.db.Where("model_name = ? AND status = ?", modelName, "production").
        Order("timestamp DESC").
        First(&version).Error
    
    return &version, err
}
```

## 📊 Performance Optimization

### Model Caching
```go
type ModelCache struct {
    cache    *cache.Cache
    ttl      time.Duration
    maxSize  int
}

func (mc *ModelCache) Get(key string) (interface{}, error) {
    if cached, found := mc.cache.Get(key); found {
        return cached, nil
    }
    return nil, errors.New("not found")
}

func (mc *ModelCache) Set(key string, value interface{}) error {
    return mc.cache.Set(key, value, mc.ttl)
}

func (mc *ModelCache) Invalidate(pattern string) error {
    // Invalidate all keys matching pattern
    keys := mc.cache.GetKeys(pattern)
    for _, key := range keys {
        mc.cache.Delete(key)
    }
    return nil
}
```

### Batch Processing
```go
type BatchProcessor struct {
    batchSize    int
    flushInterval time.Duration
    processor    func([]interface{}) error
    buffer       []interface{}
    mutex        sync.Mutex
}

func (bp *BatchProcessor) Process(item interface{}) error {
    bp.mutex.Lock()
    defer bp.mutex.Unlock()
    
    bp.buffer = append(bp.buffer, item)
    
    if len(bp.buffer) >= bp.batchSize {
        return bp.flush()
    }
    
    return nil
}

func (bp *BatchProcessor) flush() error {
    if len(bp.buffer) == 0 {
        return nil
    }
    
    batch := make([]interface{}, len(bp.buffer))
    copy(batch, bp.buffer)
    bp.buffer = bp.buffer[:0]
    
    return bp.processor(batch)
}
```

## 🔍 Monitoring and Observability

### ML Metrics
```go
type MLMetrics struct {
    accuracy     prometheus.Gauge
    latency      prometheus.Histogram
    throughput   prometheus.Counter
    predictions  prometheus.Counter
    errors       prometheus.Counter
}

func NewMLMetrics() *MLMetrics {
    return &MLMetrics{
        accuracy: prometheus.NewGauge(prometheus.GaugeOpts{
            Name: "ml_model_accuracy",
            Help: "Model accuracy",
        }),
        latency: prometheus.NewHistogram(prometheus.HistogramOpts{
            Name: "ml_prediction_latency_seconds",
            Help: "Prediction latency",
        }),
        throughput: prometheus.NewCounter(prometheus.CounterOpts{
            Name: "ml_predictions_total",
            Help: "Total predictions",
        }),
        predictions: prometheus.NewCounter(prometheus.CounterOpts{
            Name: "ml_predictions_total",
            Help: "Total predictions",
        }),
        errors: prometheus.NewCounter(prometheus.CounterOpts{
            Name: "ml_errors_total",
            Help: "Total errors",
        }),
    }
}
```

## 🎯 Best Practices

### Model Development
1. **Version Control**: Track all model versions
2. **Testing**: Comprehensive model testing
3. **Documentation**: Clear model documentation
4. **Monitoring**: Continuous model monitoring
5. **Rollback**: Quick rollback capabilities

### Production Deployment
1. **Gradual Rollout**: Canary deployments
2. **Health Checks**: Model health monitoring
3. **Performance**: Latency and throughput monitoring
4. **Security**: Model and data security
5. **Compliance**: Regulatory compliance

---

**Last Updated**: December 2024  
**Category**: Advanced ML Systems  
**Complexity**: Senior Level
