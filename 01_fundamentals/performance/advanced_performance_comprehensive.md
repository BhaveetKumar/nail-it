# Advanced Performance Comprehensive

Comprehensive guide to advanced performance optimization for senior backend engineers.

## ðŸŽ¯ Advanced Performance Profiling

### CPU Profiling
```go
// Advanced CPU Profiling Implementation
package performance

import (
    "context"
    "fmt"
    "runtime"
    "runtime/pprof"
    "sync"
    "time"
)

type CPUProfiler struct {
    enabled    bool
    outputFile string
    mutex      sync.RWMutex
}

func NewCPUProfiler(outputFile string) *CPUProfiler {
    return &CPUProfiler{
        enabled:    false,
        outputFile: outputFile,
    }
}

func (cp *CPUProfiler) Start() error {
    cp.mutex.Lock()
    defer cp.mutex.Unlock()
    
    if cp.enabled {
        return fmt.Errorf("profiler already running")
    }
    
    file, err := os.Create(cp.outputFile)
    if err != nil {
        return fmt.Errorf("failed to create profile file: %w", err)
    }
    defer file.Close()
    
    if err := pprof.StartCPUProfile(file); err != nil {
        return fmt.Errorf("failed to start CPU profile: %w", err)
    }
    
    cp.enabled = true
    return nil
}

func (cp *CPUProfiler) Stop() error {
    cp.mutex.Lock()
    defer cp.mutex.Unlock()
    
    if !cp.enabled {
        return fmt.Errorf("profiler not running")
    }
    
    pprof.StopCPUProfile()
    cp.enabled = false
    return nil
}

func (cp *CPUProfiler) IsEnabled() bool {
    cp.mutex.RLock()
    defer cp.mutex.RUnlock()
    return cp.enabled
}

// Memory Profiling
type MemoryProfiler struct {
    enabled    bool
    outputFile string
    mutex      sync.RWMutex
}

func NewMemoryProfiler(outputFile string) *MemoryProfiler {
    return &MemoryProfiler{
        enabled:    false,
        outputFile: outputFile,
    }
}

func (mp *MemoryProfiler) Start() error {
    mp.mutex.Lock()
    defer mp.mutex.Unlock()
    
    if mp.enabled {
        return fmt.Errorf("profiler already running")
    }
    
    mp.enabled = true
    return nil
}

func (mp *MemoryProfiler) Stop() error {
    mp.mutex.Lock()
    defer mp.mutex.Unlock()
    
    if !mp.enabled {
        return fmt.Errorf("profiler not running")
    }
    
    file, err := os.Create(mp.outputFile)
    if err != nil {
        return fmt.Errorf("failed to create profile file: %w", err)
    }
    defer file.Close()
    
    if err := pprof.WriteHeapProfile(file); err != nil {
        return fmt.Errorf("failed to write heap profile: %w", err)
    }
    
    mp.enabled = false
    return nil
}

func (mp *MemoryProfiler) IsEnabled() bool {
    mp.mutex.RLock()
    defer mp.mutex.RUnlock()
    return mp.enabled
}

// Goroutine Profiling
type GoroutineProfiler struct {
    enabled    bool
    outputFile string
    mutex      sync.RWMutex
}

func NewGoroutineProfiler(outputFile string) *GoroutineProfiler {
    return &GoroutineProfiler{
        enabled:    false,
        outputFile: outputFile,
    }
}

func (gp *GoroutineProfiler) Start() error {
    gp.mutex.Lock()
    defer gp.mutex.Unlock()
    
    if gp.enabled {
        return fmt.Errorf("profiler already running")
    }
    
    gp.enabled = true
    return nil
}

func (gp *GoroutineProfiler) Stop() error {
    gp.mutex.Lock()
    defer gp.mutex.Unlock()
    
    if !gp.enabled {
        return fmt.Errorf("profiler not running")
    }
    
    file, err := os.Create(gp.outputFile)
    if err != nil {
        return fmt.Errorf("failed to create profile file: %w", err)
    }
    defer file.Close()
    
    if err := pprof.Lookup("goroutine").WriteTo(file, 0); err != nil {
        return fmt.Errorf("failed to write goroutine profile: %w", err)
    }
    
    gp.enabled = false
    return nil
}

func (gp *GoroutineProfiler) IsEnabled() bool {
    gp.mutex.RLock()
    defer gp.mutex.RUnlock()
    return gp.enabled
}

// Block Profiling
type BlockProfiler struct {
    enabled    bool
    outputFile string
    mutex      sync.RWMutex
}

func NewBlockProfiler(outputFile string) *BlockProfiler {
    return &BlockProfiler{
        enabled:    false,
        outputFile: outputFile,
    }
}

func (bp *BlockProfiler) Start() error {
    bp.mutex.Lock()
    defer bp.mutex.Unlock()
    
    if bp.enabled {
        return fmt.Errorf("profiler already running")
    }
    
    runtime.SetBlockProfileRate(1) // Profile every block event
    bp.enabled = true
    return nil
}

func (bp *BlockProfiler) Stop() error {
    bp.mutex.Lock()
    defer bp.mutex.Unlock()
    
    if !bp.enabled {
        return fmt.Errorf("profiler not running")
    }
    
    file, err := os.Create(bp.outputFile)
    if err != nil {
        return fmt.Errorf("failed to create profile file: %w", err)
    }
    defer file.Close()
    
    if err := pprof.Lookup("block").WriteTo(file, 0); err != nil {
        return fmt.Errorf("failed to write block profile: %w", err)
    }
    
    runtime.SetBlockProfileRate(0) // Disable block profiling
    bp.enabled = false
    return nil
}

func (bp *BlockProfiler) IsEnabled() bool {
    bp.mutex.RLock()
    defer bp.mutex.RUnlock()
    return bp.enabled
}
```

### Advanced Benchmarking
```go
// Advanced Benchmarking Implementation
package performance

import (
    "context"
    "fmt"
    "runtime"
    "sync"
    "testing"
    "time"
)

type BenchmarkSuite struct {
    benchmarks map[string]*Benchmark
    mutex      sync.RWMutex
}

type Benchmark struct {
    Name        string
    Function    func() error
    Iterations  int
    Duration    time.Duration
    Memory      bool
    CPU         bool
    Results     *BenchmarkResults
}

type BenchmarkResults struct {
    Name           string        `json:"name"`
    Iterations     int           `json:"iterations"`
    Duration       time.Duration `json:"duration"`
    AvgDuration    time.Duration `json:"avg_duration"`
    MinDuration    time.Duration `json:"min_duration"`
    MaxDuration    time.Duration `json:"max_duration"`
    MemoryAllocs   int64         `json:"memory_allocs"`
    MemoryBytes    int64         `json:"memory_bytes"`
    CPUPercent     float64       `json:"cpu_percent"`
    Throughput     float64       `json:"throughput"`
    ErrorRate      float64       `json:"error_rate"`
    Errors         int           `json:"errors"`
}

func NewBenchmarkSuite() *BenchmarkSuite {
    return &BenchmarkSuite{
        benchmarks: make(map[string]*Benchmark),
    }
}

func (bs *BenchmarkSuite) AddBenchmark(name string, fn func() error, iterations int, duration time.Duration) {
    bs.mutex.Lock()
    defer bs.mutex.Unlock()
    
    bs.benchmarks[name] = &Benchmark{
        Name:       name,
        Function:   fn,
        Iterations: iterations,
        Duration:   duration,
        Memory:     true,
        CPU:        true,
    }
}

func (bs *BenchmarkSuite) RunBenchmark(name string) (*BenchmarkResults, error) {
    bs.mutex.RLock()
    benchmark, exists := bs.benchmarks[name]
    bs.mutex.RUnlock()
    
    if !exists {
        return nil, fmt.Errorf("benchmark %s not found", name)
    }
    
    return bs.runBenchmark(benchmark)
}

func (bs *BenchmarkSuite) RunAllBenchmarks() (map[string]*BenchmarkResults, error) {
    bs.mutex.RLock()
    benchmarks := make(map[string]*Benchmark)
    for name, benchmark := range bs.benchmarks {
        benchmarks[name] = benchmark
    }
    bs.mutex.RUnlock()
    
    results := make(map[string]*BenchmarkResults)
    
    for name, benchmark := range benchmarks {
        result, err := bs.runBenchmark(benchmark)
        if err != nil {
            return nil, fmt.Errorf("benchmark %s failed: %w", name, err)
        }
        results[name] = result
    }
    
    return results, nil
}

func (bs *BenchmarkSuite) runBenchmark(benchmark *Benchmark) (*BenchmarkResults, error) {
    var durations []time.Duration
    var errors int
    var totalDuration time.Duration
    
    // Memory profiling
    var memStatsBefore, memStatsAfter runtime.MemStats
    if benchmark.Memory {
        runtime.GC()
        runtime.ReadMemStats(&memStatsBefore)
    }
    
    // CPU profiling
    var cpuStart time.Time
    if benchmark.CPU {
        cpuStart = time.Now()
    }
    
    // Run benchmark
    start := time.Now()
    for i := 0; i < benchmark.Iterations; i++ {
        iterStart := time.Now()
        
        if err := benchmark.Function(); err != nil {
            errors++
        }
        
        iterDuration := time.Since(iterStart)
        durations = append(durations, iterDuration)
        totalDuration += iterDuration
        
        // Check if we've exceeded the duration limit
        if time.Since(start) >= benchmark.Duration {
            break
        }
    }
    
    // Memory profiling
    if benchmark.Memory {
        runtime.GC()
        runtime.ReadMemStats(&memStatsAfter)
    }
    
    // Calculate results
    results := &BenchmarkResults{
        Name:        benchmark.Name,
        Iterations:  len(durations),
        Duration:    time.Since(start),
        Errors:      errors,
        ErrorRate:   float64(errors) / float64(len(durations)),
    }
    
    if len(durations) > 0 {
        results.AvgDuration = totalDuration / time.Duration(len(durations))
        results.MinDuration = durations[0]
        results.MaxDuration = durations[0]
        
        for _, duration := range durations {
            if duration < results.MinDuration {
                results.MinDuration = duration
            }
            if duration > results.MaxDuration {
                results.MaxDuration = duration
            }
        }
        
        results.Throughput = float64(len(durations)) / results.Duration.Seconds()
    }
    
    if benchmark.Memory {
        results.MemoryAllocs = memStatsAfter.TotalAlloc - memStatsBefore.TotalAlloc
        results.MemoryBytes = memStatsAfter.Alloc - memStatsBefore.Alloc
    }
    
    if benchmark.CPU {
        cpuDuration := time.Since(cpuStart)
        results.CPUPercent = (cpuDuration.Seconds() / results.Duration.Seconds()) * 100
    }
    
    return results, nil
}

// Load Testing
type LoadTester struct {
    concurrency int
    duration    time.Duration
    rampUp      time.Duration
    rampDown    time.Duration
    mutex       sync.RWMutex
}

type LoadTestConfig struct {
    Concurrency int           `json:"concurrency"`
    Duration    time.Duration `json:"duration"`
    RampUp      time.Duration `json:"ramp_up"`
    RampDown    time.Duration `json:"ramp_down"`
}

type LoadTestResults struct {
    Config      LoadTestConfig `json:"config"`
    StartTime   time.Time      `json:"start_time"`
    EndTime     time.Time      `json:"end_time"`
    Duration    time.Duration  `json:"duration"`
    Requests    int            `json:"requests"`
    Errors      int            `json:"errors"`
    SuccessRate float64        `json:"success_rate"`
    Throughput  float64        `json:"throughput"`
    AvgLatency  time.Duration  `json:"avg_latency"`
    MinLatency  time.Duration  `json:"min_latency"`
    MaxLatency  time.Duration  `json:"max_latency"`
    P50Latency  time.Duration  `json:"p50_latency"`
    P90Latency  time.Duration  `json:"p90_latency"`
    P95Latency  time.Duration  `json:"p95_latency"`
    P99Latency  time.Duration  `json:"p99_latency"`
}

func NewLoadTester(config LoadTestConfig) *LoadTester {
    return &LoadTester{
        concurrency: config.Concurrency,
        duration:    config.Duration,
        rampUp:      config.RampUp,
        rampDown:    config.RampDown,
    }
}

func (lt *LoadTester) RunLoadTest(ctx context.Context, fn func() error) (*LoadTestResults, error) {
    startTime := time.Now()
    
    var wg sync.WaitGroup
    var mu sync.Mutex
    var latencies []time.Duration
    var requests int
    var errors int
    
    // Create workers
    workerChan := make(chan struct{}, lt.concurrency)
    
    // Ramp up
    go func() {
        for i := 0; i < lt.concurrency; i++ {
            select {
            case <-ctx.Done():
                return
            case <-time.After(lt.rampUp / time.Duration(lt.concurrency)):
                workerChan <- struct{}{}
            }
        }
    }()
    
    // Run load test
    go func() {
        defer close(workerChan)
        
        for {
            select {
            case <-ctx.Done():
                return
            case <-time.After(lt.duration):
                return
            case <-workerChan:
                wg.Add(1)
                go func() {
                    defer wg.Done()
                    
                    iterStart := time.Now()
                    err := fn()
                    latency := time.Since(iterStart)
                    
                    mu.Lock()
                    requests++
                    if err != nil {
                        errors++
                    }
                    latencies = append(latencies, latency)
                    mu.Unlock()
                }()
            }
        }
    }()
    
    // Wait for completion
    wg.Wait()
    
    endTime := time.Now()
    
    // Calculate results
    results := &LoadTestResults{
        Config: LoadTestConfig{
            Concurrency: lt.concurrency,
            Duration:    lt.duration,
            RampUp:      lt.rampUp,
            RampDown:    lt.rampDown,
        },
        StartTime:   startTime,
        EndTime:     endTime,
        Duration:    endTime.Sub(startTime),
        Requests:    requests,
        Errors:      errors,
        SuccessRate: float64(requests-errors) / float64(requests),
        Throughput:  float64(requests) / endTime.Sub(startTime).Seconds(),
    }
    
    if len(latencies) > 0 {
        // Sort latencies
        sort.Slice(latencies, func(i, j int) bool {
            return latencies[i] < latencies[j]
        })
        
        // Calculate latency statistics
        var totalLatency time.Duration
        for _, latency := range latencies {
            totalLatency += latency
        }
        
        results.AvgLatency = totalLatency / time.Duration(len(latencies))
        results.MinLatency = latencies[0]
        results.MaxLatency = latencies[len(latencies)-1]
        
        // Calculate percentiles
        results.P50Latency = latencies[int(float64(len(latencies))*0.5)]
        results.P90Latency = latencies[int(float64(len(latencies))*0.9)]
        results.P95Latency = latencies[int(float64(len(latencies))*0.95)]
        results.P99Latency = latencies[int(float64(len(latencies))*0.99)]
    }
    
    return results, nil
}
```

### Advanced Caching Strategies
```go
// Advanced Caching Strategies Implementation
package performance

import (
    "context"
    "fmt"
    "sync"
    "time"
)

// Multi-Level Cache
type MultiLevelCache struct {
    l1Cache *L1Cache
    l2Cache *L2Cache
    l3Cache *L3Cache
    mutex   sync.RWMutex
}

type L1Cache struct {
    data map[string]*CacheItem
    mutex sync.RWMutex
    ttl   time.Duration
}

type L2Cache struct {
    data map[string]*CacheItem
    mutex sync.RWMutex
    ttl   time.Duration
}

type L3Cache struct {
    data map[string]*CacheItem
    mutex sync.RWMutex
    ttl   time.Duration
}

type CacheItem struct {
    Value     interface{}
    ExpiresAt time.Time
    AccessCount int
    LastAccess time.Time
}

func NewMultiLevelCache(l1TTL, l2TTL, l3TTL time.Duration) *MultiLevelCache {
    return &MultiLevelCache{
        l1Cache: NewL1Cache(l1TTL),
        l2Cache: NewL2Cache(l2TTL),
        l3Cache: NewL3Cache(l3TTL),
    }
}

func (mlc *MultiLevelCache) Get(ctx context.Context, key string) (interface{}, bool) {
    // Try L1 cache first
    if value, found := mlc.l1Cache.Get(key); found {
        return value, true
    }
    
    // Try L2 cache
    if value, found := mlc.l2Cache.Get(key); found {
        // Promote to L1
        mlc.l1Cache.Set(key, value)
        return value, true
    }
    
    // Try L3 cache
    if value, found := mlc.l3Cache.Get(key); found {
        // Promote to L2 and L1
        mlc.l2Cache.Set(key, value)
        mlc.l1Cache.Set(key, value)
        return value, true
    }
    
    return nil, false
}

func (mlc *MultiLevelCache) Set(ctx context.Context, key string, value interface{}) {
    // Set in all levels
    mlc.l1Cache.Set(key, value)
    mlc.l2Cache.Set(key, value)
    mlc.l3Cache.Set(key, value)
}

func (mlc *MultiLevelCache) Delete(ctx context.Context, key string) {
    mlc.l1Cache.Delete(key)
    mlc.l2Cache.Delete(key)
    mlc.l3Cache.Delete(key)
}

// L1 Cache Implementation
func NewL1Cache(ttl time.Duration) *L1Cache {
    cache := &L1Cache{
        data: make(map[string]*CacheItem),
        ttl:  ttl,
    }
    
    // Start cleanup goroutine
    go cache.cleanup()
    
    return cache
}

func (l1 *L1Cache) Get(key string) (interface{}, bool) {
    l1.mutex.RLock()
    defer l1.mutex.RUnlock()
    
    item, exists := l1.data[key]
    if !exists {
        return nil, false
    }
    
    if time.Now().After(item.ExpiresAt) {
        return nil, false
    }
    
    // Update access statistics
    item.AccessCount++
    item.LastAccess = time.Now()
    
    return item.Value, true
}

func (l1 *L1Cache) Set(key string, value interface{}) {
    l1.mutex.Lock()
    defer l1.mutex.Unlock()
    
    l1.data[key] = &CacheItem{
        Value:      value,
        ExpiresAt:  time.Now().Add(l1.ttl),
        AccessCount: 1,
        LastAccess: time.Now(),
    }
}

func (l1 *L1Cache) Delete(key string) {
    l1.mutex.Lock()
    defer l1.mutex.Unlock()
    
    delete(l1.data, key)
}

func (l1 *L1Cache) cleanup() {
    ticker := time.NewTicker(1 * time.Minute)
    defer ticker.Stop()
    
    for range ticker.C {
        l1.mutex.Lock()
        now := time.Now()
        for key, item := range l1.data {
            if now.After(item.ExpiresAt) {
                delete(l1.data, key)
            }
        }
        l1.mutex.Unlock()
    }
}

// L2 Cache Implementation
func NewL2Cache(ttl time.Duration) *L2Cache {
    cache := &L2Cache{
        data: make(map[string]*CacheItem),
        ttl:  ttl,
    }
    
    // Start cleanup goroutine
    go cache.cleanup()
    
    return cache
}

func (l2 *L2Cache) Get(key string) (interface{}, bool) {
    l2.mutex.RLock()
    defer l2.mutex.RUnlock()
    
    item, exists := l2.data[key]
    if !exists {
        return nil, false
    }
    
    if time.Now().After(item.ExpiresAt) {
        return nil, false
    }
    
    // Update access statistics
    item.AccessCount++
    item.LastAccess = time.Now()
    
    return item.Value, true
}

func (l2 *L2Cache) Set(key string, value interface{}) {
    l2.mutex.Lock()
    defer l2.mutex.Unlock()
    
    l2.data[key] = &CacheItem{
        Value:      value,
        ExpiresAt:  time.Now().Add(l2.ttl),
        AccessCount: 1,
        LastAccess: time.Now(),
    }
}

func (l2 *L2Cache) Delete(key string) {
    l2.mutex.Lock()
    defer l2.mutex.Unlock()
    
    delete(l2.data, key)
}

func (l2 *L2Cache) cleanup() {
    ticker := time.NewTicker(5 * time.Minute)
    defer ticker.Stop()
    
    for range ticker.C {
        l2.mutex.Lock()
        now := time.Now()
        for key, item := range l2.data {
            if now.After(item.ExpiresAt) {
                delete(l2.data, key)
            }
        }
        l2.mutex.Unlock()
    }
}

// L3 Cache Implementation
func NewL3Cache(ttl time.Duration) *L3Cache {
    cache := &L3Cache{
        data: make(map[string]*CacheItem),
        ttl:  ttl,
    }
    
    // Start cleanup goroutine
    go cache.cleanup()
    
    return cache
}

func (l3 *L3Cache) Get(key string) (interface{}, bool) {
    l3.mutex.RLock()
    defer l3.mutex.RUnlock()
    
    item, exists := l3.data[key]
    if !exists {
        return nil, false
    }
    
    if time.Now().After(item.ExpiresAt) {
        return nil, false
    }
    
    // Update access statistics
    item.AccessCount++
    item.LastAccess = time.Now()
    
    return item.Value, true
}

func (l3 *L3Cache) Set(key string, value interface{}) {
    l3.mutex.Lock()
    defer l3.mutex.Unlock()
    
    l3.data[key] = &CacheItem{
        Value:      value,
        ExpiresAt:  time.Now().Add(l3.ttl),
        AccessCount: 1,
        LastAccess: time.Now(),
    }
}

func (l3 *L3Cache) Delete(key string) {
    l3.mutex.Lock()
    defer l3.mutex.Unlock()
    
    delete(l3.data, key)
}

func (l3 *L3Cache) cleanup() {
    ticker := time.NewTicker(10 * time.Minute)
    defer ticker.Stop()
    
    for range ticker.C {
        l3.mutex.Lock()
        now := time.Now()
        for key, item := range l3.data {
            if now.After(item.ExpiresAt) {
                delete(l3.data, key)
            }
        }
        l3.mutex.Unlock()
    }
}
```

## ðŸŽ¯ Best Practices

### Performance Principles
1. **Measure First**: Always measure before optimizing
2. **Profile Guided**: Use profiling to identify bottlenecks
3. **Incremental**: Optimize incrementally and measure impact
4. **Context Aware**: Consider the context and requirements
5. **Trade-offs**: Understand the trade-offs of optimizations

### Optimization Strategies
1. **Algorithmic**: Choose the right algorithms and data structures
2. **Caching**: Implement appropriate caching strategies
3. **Concurrency**: Use concurrency and parallelism effectively
4. **Memory**: Optimize memory usage and allocation patterns
5. **I/O**: Optimize I/O operations and reduce blocking

### Monitoring and Profiling
1. **Continuous Monitoring**: Monitor performance continuously
2. **Profiling**: Use profiling tools to identify bottlenecks
3. **Benchmarking**: Create and maintain performance benchmarks
4. **Alerting**: Set up performance alerts and thresholds
5. **Documentation**: Document performance characteristics and optimizations

---

**Last Updated**: December 2024  
**Category**: Advanced Performance Comprehensive  
**Complexity**: Expert Level
